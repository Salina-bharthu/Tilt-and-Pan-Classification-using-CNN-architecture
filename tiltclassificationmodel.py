# -*- coding: utf-8 -*-
"""TiltClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SR7RlyJkTsR4mbwdxyKVSgO2ReBVwR4c

**Deep Learning (COSC2779)  Assignment 1**

Salina Bharthu (S3736867)

Aim:
The aim of this assignment is to develope a deep convolutional neural network(CNN) to identify the head pose given an image of a person.

This Notebook outlines the implementation of deep CNN Model for the classification of tilt angles of images.
"""

# set this to print all output from a cell in notebook, not just the most recent one
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
import warnings   #suppress warnings
warnings.filterwarnings("ignore")

#Import useful Libraries
import pandas as pd
import numpy as np
import tensorflow as tf
import os
import cv2
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow_addons as tfa
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Lambda, Input, BatchNormalization
from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense
from tensorflow.keras.metrics import categorical_accuracy
from tensorflow.keras import regularizers, optimizers
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.metrics import classification_report, confusion_matrix

"""#**Data Extraction and Exploration**

Dataset: 
The Dataset contains images of headpose captured from the camera placed on front. The headpose is quantified in two different labels, that are, tilt(vertical angle of the head) and pan(horizontal angle of the head). 
The images are having labels of tilt assigned in seperate csv data files along with person_id and series_id.
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

#Extracting Image Files from drive
!cp /content/drive/'My Drive'/'Colab Notebooks'/modified_data.zip .
!unzip -q -o modified_data.zip -d Imagedata
!rm modified_data.zip

#Extracting csv files containing metadata about images
data = pd.read_csv("../content/drive/My Drive/Colab Notebooks/train_data.csv")
testdata = pd.read_csv("../content/drive/My Drive/Colab Notebooks/test_data.csv")

print("Data frame Size:", len(data) ,"\n")
print("Tilt Labels:")
data.tilt.unique()
data["tilt"] = data["tilt"].astype(str) 

print("\n\nPerson_id Distribution in training dataframe")
data.person_id.value_counts()

print("\n\nSeries Distribution in training dataframe")
data.series.value_counts()

#Plotting the Distribution of target labels
categorical_features = ["tilt"]
fig = plt.figure(figsize=(10,10))
for i, categorical_feature in enumerate(data[categorical_features]):
    data[categorical_feature].value_counts().plot(kind = "bar", figsize = (15,5)).set_title(categorical_feature)
fig.show()

"""The above plot shows that, except two target variable class 90 and -90, all other target variable classes are balanced with equal data distribution. This can be addressed by adding  in using Augmented Image data."""

#Extract image for each Label of Tilt Angle

tilt_label = set(data['tilt'])
plt.figure(figsize=(20, 20))
i = 1 #counter
path = "/content/Imagedata/modified_data/"

for label in tilt_label:
    fullpath = path + data.loc[list(data['tilt']).index(label)][0] #path to first image of each label
    image = cv2.imread(fullpath)
    plt.subplot(6, 6, i)
    plt.axis('off')
    plt.title("Tilt: {0}".format(label, list(data['tilt']).count(label)))
    i += 1
    plt.imshow(image, cmap="gray")

plt.show()

#test dataframe exploration
len(testdata)
testdata.person_id.unique()
testdata.series.unique()

#Finding suitable Image size to feed into network
image_name = data['filename'][0]
full_path = "/content/Imagedata/modified_data/" + image_name
image = cv2.imread(full_path, cv2.IMREAD_GRAYSCALE)
print("\nImage Size:" , image.shape)

plt.imshow(image, cmap = "gray")
plt.show()

image = cv2.resize(image, (100,100))  
plt.imshow(image, cmap = "gray")
plt.show()

image = cv2.resize(image, (90,90))
plt.imshow(image, cmap = "gray")
plt.show()

"""#**Tilt Classification Model Implementation**



**Data Preparation**


*   Initially, the train and test dataframes are seprated using person_id. The train dataframe is further splitted into train-validation datasets of 80-20 ratio.

*   While splitting the train and validation data, stratify is used on the target column in-order to ensure equalt distribution of images from all classes.

*   The train and validation dataframe is further used to extract training and validation images using filenaes. This is implemented using keras ImageDataGenerator at runtime.
*   The images are extracted using target_size of 100*100, which is finalized after experimenting with different sizes
"""

#Train-validation-test dataframe split
train_df  = data.loc[data['person_id'] < 13]
test_df = data.loc[data['person_id'] >= 13]
train_df.tilt.unique()

train_df_tilt, val_df_tilt = train_test_split(train_df, test_size= 0.2, stratify = train_df['tilt'], random_state = 22)

#Preparing Training and Validation datasets
path = "/content/Imagedata/modified_data"
BATCH_SIZE = 16 

def tilt_train_generator():
    datagen = ImageDataGenerator(data_format='channels_last', rescale=1./255)
    tilt_train_generator = datagen.flow_from_dataframe(
        dataframe = train_df_tilt,
        directory = path,
        x_col = "filename",
        y_col = "tilt",
        target_size = (100, 100),
        batch_size = 1,
        color_mode = "grayscale",
        shuffle = False,
        class_mode = 'categorical')
    return tilt_train_generator

def tilt_val_generator():
    datagen = ImageDataGenerator(data_format='channels_last', rescale=1./255)
    tilt_val_generator = datagen.flow_from_dataframe(
        dataframe = val_df_tilt,
        directory = path,
        x_col = "filename",
        y_col = "tilt",
        target_size = (100, 100),
        batch_size = 1,
        color_mode = "grayscale",
        shuffle = False,
        class_mode = 'categorical')
    return tilt_val_generator

train_dataset = tf.data.Dataset.from_generator(tilt_train_generator, output_types=(tf.float32, tf.float32), output_shapes=([1,100,100,1], [1,9]))
vaidation_dataset = tf.data.Dataset.from_generator(tilt_val_generator,output_types=(tf.float32, tf.float32), output_shapes=([1,100,100,1], [1,9]))

VAL_DATA_LEN = val_df_tilt.shape[0]
TRAIN_DATA_LEN = train_df_tilt.shape[0]

AUTOTUNE = tf.data.experimental.AUTOTUNE

def convert(image, label):
  image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1]
  image = image[0,:]
  label = label[0,:]
  return image, label

def augment(image,label):
  image,label = convert(image, label)
  image = tf.image.resize_with_crop_or_pad(image, 110, 110) # Add 10 pixels of padding
  image = tf.image.random_brightness(image, max_delta=0.3) # Random brightness
  image = tf.image.random_crop(image, size=[100, 100, 1]) # crop back to 100*100
  
  return image,label

augmented_train_batches = train_dataset.take(TRAIN_DATA_LEN).cache()
augmented_train_batches = augmented_train_batches.shuffle(TRAIN_DATA_LEN).map(augment, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE, drop_remainder=True)

non_augmented_train_batches = train_dataset.take(TRAIN_DATA_LEN).cache()
non_augmented_train_batches = non_augmented_train_batches.shuffle(TRAIN_DATA_LEN).map(convert, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE, drop_remainder=True)

validation_batches = vaidation_dataset.take(VAL_DATA_LEN).cache()
validation_batches =validation_batches.map(convert, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE, drop_remainder=True)

"""#**Baseline CNN Model**





*   Initially, developing very basic CNN model for dry run with 2 convolutional layers followed by max pooling layers and fully connected layers at the end for classification of 9 tilt angles
*   The image with input size of 100*100 with depth = 1 (grayscale images) are given as an input to the Convolutional layers
*   Using 'relu' activation for adding non-linearity and 'adam' optimizer for faster convergence and adaptive learning rate
*   Training the base model on Augmented and Non-augmented Datasets with 50 epochs
*   Using categorical_crossentropy and accuracy metrics to evaluate the model performance
"""

# Function to create Neural Network Model
def base_model_cnn(num_label):
    
    classifier = Sequential()
    
    # Input layer
    classifier.add(Input(shape=(100, 100, 1)))

    # Conv Layer 1
    classifier.add(Conv2D(16, (3, 3), activation='relu'))
    classifier.add(MaxPooling2D(pool_size=(2, 2)))

    # Conv Layer 2 (no pooling)
    classifier.add(Conv2D(32, (3, 3), activation='relu'))
    classifier.add(MaxPooling2D(pool_size=(2, 2)))

    #Fully Connected layer
    classifier.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors
    classifier.add(Dense(64, activation='relu'))

    #output layer
    classifier.add(Dense(num_label, activation='softmax'))    
    classifier.summary()

    return classifier

UNIQUE_TILT = len(set(train_df_tilt['tilt']))

base_histories = {}
model_with_aug_tilt = base_model_cnn(UNIQUE_TILT)  
model_with_aug_tilt.compile(optimizer = tf.keras.optimizers.Adam(), loss=tf.losses.CategoricalCrossentropy(),
              metrics=[tf.losses.CategoricalCrossentropy(name='CategoricalCrossentropy'), 'accuracy'])

#On Non Augmented Data
base_histories['Without_Aug_tilt'] = model_with_aug_tilt.fit(non_augmented_train_batches, epochs=50, validation_data=validation_batches, verbose=1, batch_size = BATCH_SIZE)

#On Augmented Data
base_histories['With_Aug_tilt'] = model_with_aug_tilt.fit(augmented_train_batches, epochs=50, validation_data=validation_batches, verbose=1, batch_size = BATCH_SIZE)

#Function to plot validation curves
from itertools import cycle
def plotter(history_hold, metric = 'binary_crossentropy', ylim=[0.0, 2.0]):
  cycol = cycle('bgrcmk')
  for name, item in history_hold.items():
    y_train = item.history[metric]
    y_val = item.history['val_' + metric]
    x_train = np.arange(0,len(y_val))

    c=next(cycol)

    plt.plot(x_train, y_train, c+'-', label=name+'_train')
    plt.plot(x_train, y_val, c+'--', label=name+'_val')

  plt.legend()
  plt.xlim([1, max(plt.xlim())])
  plt.ylim(ylim)
  plt.xlabel('Epoch')
  plt.ylabel(metric)
  plt.grid(True)

plotter(base_histories, ylim=[0.0, 2.0], metric = 'CategoricalCrossentropy')

plotter(base_histories, ylim=[0.0, 1.1], metric = 'accuracy')

"""*   The above two learning and validation curve shows that model gives very high generalization gap and overfitting of data while using non augmented training batches. Therefore, using augmented training batches for further implementation
*   Also, the augmented batches shows the minor signs of overfitting and fluctuations

**CNN Model Architecture Modifications**

*   L2 Kernel regularizer is added to Convolution layer to deal with the minor overfitting. In this task, L2 regularizer adds small penalty value to the weights matrix of the nodes. Initially 0.001 regularization coefficient value is use which is optimized to obtain well-fitted model. The regularization penalty is very small as there is no obvious sign of overfitting at the moment but by further increasing the model complexity, it can be useful to avoid the issue of overfitting.

*   In fully connected layers, dropout layer is added. It zero out the activation of randomly chosen nuerons as per the dropout rate. The dropout rate is initially chosen as 0.5 and further optimized. It can help reduce the runtime and avoid the overfitting.

*   To enhance the accuracy, 1 (Convolution + max pooling) layers are added. The filter size of the layers near the input layer is small and incremently changing in later layers.

*   One additional fully connected layer is added. The number of nuerons are optimized later. 

*   The 'adam' optimizer is used with decaying learnng rate at run time. The adam optimizer itself uses adaptive learning rate to update the weights of each parameter, however, using exponential decay helps to speed up the optimization process.

* Here, the evaluation metrics precision and recall are added to gain better understanding of multi class classification
"""

def get_cnn_model(num_label, STEPS_PER_EPOCH, filters = 8, kernel_size = 3, pool_size = 2, lambda_reg = 0.01, dropout = 0.5, units = 64, initial_lr= 0.001):
  classifier = Sequential()

  # input
  classifier.add(Input(shape=(100, 100, 1)))

  # Conv Layer 1
  classifier.add(Conv2D(filters, (kernel_size, kernel_size), activation='relu', padding='same', kernel_regularizer=regularizers.l2(lambda_reg)))
  classifier.add(MaxPooling2D(pool_size=(pool_size, pool_size)))

  # Conv Layer 2 (no pooling)
  classifier.add(Conv2D(filters*2, (kernel_size, kernel_size), activation='relu', padding='same', kernel_regularizer=regularizers.l2(lambda_reg)))
  classifier.add(MaxPooling2D(pool_size=(pool_size, pool_size)))

  # Conv Layer 3
  classifier.add(Conv2D(filters*4, (kernel_size, kernel_size), activation='relu', padding='same'))
  classifier.add(MaxPooling2D(pool_size=(pool_size, pool_size)))

  # MLP
  classifier.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors
  classifier.add(Dropout(dropout))
  classifier.add(Dense(units, activation='relu'))
  classifier.add(Dense(units/2, activation='relu'))

  #output layer
  classifier.add(Dense(num_label, activation='softmax'))
  
  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(initial_lr, decay_steps=STEPS_PER_EPOCH*1000, decay_rate=1, staircase=False)

  classifier.compile(optimizer = tf.keras.optimizers.Adam(lr_schedule),
              loss=tf.losses.CategoricalCrossentropy(),
              metrics=[tf.losses.CategoricalCrossentropy(name = 'CategoricalCrossentropy'), 'accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])

  classifier.save_weights('modelCNN.h5')
  classifier.summary()
  
  return classifier

model_histories = {}
model_tilt = get_cnn_model(UNIQUE_TILT, TRAIN_DATA_LEN)

"""Initial model run with 100 epochs"""

model_histories['With_augmentation_Tilt'] = model_tilt.fit(augmented_train_batches, epochs=100, validation_data=validation_batches, verbose=1)

plotter(model_histories, ylim=[0.0, 1.1], metric = 'CategoricalCrossentropy')

plotter(model_histories, ylim=[0.0, 1.1], metric = 'accuracy')

"""The above validation and training curve shows that the modified CNN architecture generalizes well as validation curve follows training curve well without significant gap, however, the curves are not stabilized yet, therefore training the model with more epochs"""

model_histories['With_augmentation_Tilt'] = model_tilt.fit(augmented_train_batches, epochs=150, validation_data=validation_batches, verbose=0)

plotter(model_histories, ylim=[0.0, 1.1], metric = 'CategoricalCrossentropy')

plotter(model_histories, ylim=[0.0, 1.1], metric = 'accuracy')

"""The curves look stabilized around 150 epochs.

#**Parameter Tuning for Tilt Model**



*   To further optimize the model performance, parameter tuning is performed
*   Parameter tuning of batch_size and number of epochs is performed seperated followed by tuning of othre configurable hyper parameters of CNN architecture using randomizedsearch cross validation
"""

#Preparing datasets for parameter tuning
tilt_df = tilt_train_generator()
tiltX = np.concatenate([tilt_df.next()[0] for i in range(TRAIN_DATA_LEN)])
tilty = np.concatenate([tilt_df.next()[1] for i in range(TRAIN_DATA_LEN)])
#print(tiltX.shape)
#print(tilty.shape)

X_train, X_test, y_train, y_test = train_test_split(tiltX, tilty, test_size = 0.2)

"""The small batch size generally provide rapid learning but with high variance in classification accuracy. The model shows minor fluctuation, therefore, the batch_size is tuned initially."""

model = KerasClassifier(build_fn = get_cnn_model, verbose = 1)

#set of parameters to be tuned
param_grid = dict(
    epochs = [150],
    batch_size = [16,32],
    num_label = [9],
    TRAIN_DATA_LEN = [TRAIN_DATA_LEN]
  )

#Applying randomized search with cross validation for parameter tuning
grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv = 3, n_iter = 5)
grid_result = grid.fit(X_train, y_train)

print("Best parameter set %s" % (grid_result.best_params_))

"""The following code block contains hyper parameter tuning of multiple hyper parameters and it takes long execution time. Considerring the resource limitations ,parameter tuning with 50 epochs is implemented."""

X_train, X_test, y_train, y_test = train_test_split(tiltX, tilty, test_size = 0.2)
model = KerasClassifier(build_fn = get_cnn_model, verbose = 1)
STEPS_PER_EPOCH = TRAIN_DATA_LEN// 32
#set of parameters to be tuned
param_grid = dict(
    epochs = [50],
    initial_lr = [0.1, 0.01, 0.001],
    lambda_reg = [0.1, 0.01, 0.001],
    dropout = [0.3, 0.5],
    kernel_size = [2,3],
    filters = [8, 16, 32],
    pool_size = [2,3],
    units = [64,128],
    num_label = [9],
    STEPS_PER_EPOCH = [STEPS_PER_EPOCH]
  )

#Applying randomized search with cross validation for parameter tuning
grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv = 3)
grid_result = grid.fit(X_train, y_train)

print("Best parameter set %s" % (grid_result.best_params_))

"""Using best Param values to train the model for tilt"""

#{'units': 128, 'pool_size': 3, 'num_label': 9, 'lambda_reg': 0.001, 'kernel_size': 2, 'initial_lr': 0.01, 'filters': 8, 'epochs': 50, 'dropout': 0.3, 'STEPS_PER_EPOCH': 88}
model_tilt_tune = get_cnn_model(9, 88, 8, 2, 3, 0.001, 0.3, 128, 0.01)
tilt_histories = {}
tilt_histories['With_Aug_tilt_tune'] = model_tilt_tune.fit(augmented_train_batches, epochs=150, validation_data=validation_batches, verbose=1)

plotter(tilt_histories, ylim=[0.0, 1.1], metric = 'CategoricalCrossentropy')

plotter(tilt_histories, ylim=[0.0, 1.1], metric = 'accuracy')

"""#**Model Evaluation on Hold-out Test Data**"""

TEST_DF_LEN = test_df.shape[0]

test_datagen = ImageDataGenerator( data_format='channels_last', rescale=1./255 )
test_generator = test_datagen.flow_from_dataframe(
        dataframe = test_df, 
        directory = path,
        x_col = "filename",
        y_col = "tilt",
        target_size = (100, 100),
        batch_size = 1,
        color_mode = "grayscale",
        shuffle = False,
        class_mode = 'categorical')

pred = model_tilt_tune.predict(test_generator, verbose=1, steps=TEST_DF_LEN)

predicted_class_indices_tilt = np.argmax(pred,axis=1)
labels = (test_generator.class_indices)
labels2 = dict((v,k) for k,v in labels.items())
predictions = [labels2[k] for k in predicted_class_indices_tilt]


actual_labels = test_generator.classes
actual_test_labels = [labels2[k] for k in actual_labels]

#Function to display classification report and confusion matrix
def displayConfusionMatrix(pred_y, test_lable, target_names):
    confusion_mat= confusion_matrix(test_lable, pred_y)
    print('Classification Report')
    print(classification_report(test_lable, pred_y, target_names=target_names))
    plt.imshow(confusion_mat, cmap=plt.cm.Greys)
    plt.xlabel("Predicted labels")
    plt.ylabel("True labels")
    plt.title('Confusion matrix of Hold out Test data')
    plt.colorbar()
    plt.show()

target_names_tilt = set(test_df['tilt'])
displayConfusionMatrix(actual_test_labels, predictions, target_names_tilt)

"""#**Prediction on unseen Test Data**"""

test_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last')
final_test_generator = test_datagen.flow_from_dataframe(
    dataframe=testdata,
    directory=path,
    x_col="filename",
    target_size=(100, 100),
    batch_size=1,
    color_mode="grayscale",
    shuffle=False,
    class_mode=None
    )

pred_tilt = model_tilt_tune.predict(final_test_generator, verbose=1, steps=len(final_test_generator))
final_predicted_tilt_indices = np.argmax(pred_tilt,axis=1)

labels = (test_generator.class_indices)
labels2 = dict((v,k) for k,v in labels.items())
final_predictions = [labels2[k] for k in final_predicted_tilt_indices]

final_df = pd.DataFrame()
final_df['filename'] = testdata['filename']
final_df['tilt'] = final_predictions

final_df.head()

#Write Tilt predictions to .csv file
final_df.to_csv('../content/drive/My Drive/Colab Notebooks/s3736867_tilt.csv')